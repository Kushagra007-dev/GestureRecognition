{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSNWNapZ6dOl"
      },
      "source": [
        "# Gesture Recognition\n",
        "#### - Kushagra Sengar and Nidhi Mantri\n",
        "In this group project, we are going to experiment with Conv3d and CNN-RNN architectures that will be able to predict the 5 gestures correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b95NVVVU6dOt"
      },
      "outputs": [],
      "source": [
        "# Libraries imported\n",
        "import numpy as np\n",
        "import os\n",
        "from skimage import io\n",
        "import datetime\n",
        "import os\n",
        "from skimage.transform import resize\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZCNf8n96dOw"
      },
      "source": [
        "We set the random seed so that the results don't vary drastically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUnwMIrb6dOx"
      },
      "outputs": [],
      "source": [
        "np.random.seed(30)\n",
        "import random as rn\n",
        "rn.seed(30)\n",
        "from keras import backend as K\n",
        "import tensorflow as tf\n",
        "tf.compat.v1.set_random_seed(30)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have our zip file on drive, mounting drive to get the zip file and extracting it to get the contents of train and validation data."
      ],
      "metadata": {
        "id": "mHvTXIZaAykX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duOrT3y6Bo9y",
        "outputId": "61b8133b-31b1-45e2-9192-85a8eef80a10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "##mount the drive to get the dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile(\"/content/drive/MyDrive/Project_data.zip\", 'r')\n",
        "zip_ref.extractall(\"/content/tmp\")\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "vmW6o597nl8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TajNKl8y6dOy"
      },
      "source": [
        "In this block, you read the folder names for training and validation. You also set the `batch_size` here. Note that you set the batch size in such a way that you are able to use the GPU in full capacity. You keep increasing the batch size until the machine throws an error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YfSsjiks6dOz"
      },
      "outputs": [],
      "source": [
        "# Here we are reading train and val csv files\n",
        "train_doc = np.random.permutation(open('/content/tmp/Project_data/train.csv').readlines())\n",
        "val_doc = np.random.permutation(open('/content/tmp/Project_data/val.csv').readlines())\n",
        "\n",
        "batch_size =32\n",
        "# here x is the number of images we are experimenting with, out of 30 (total images in each video)\n",
        "x=18\n",
        "# y and z are the height and width of image\n",
        "y=84\n",
        "z=84"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvDG8qf36dO0"
      },
      "source": [
        "## Generator\n",
        "This is one of the most important part of the code. The overall structure of the generator has been given. In the generator, we are going to preprocess the images as we have images of 2 different dimensions as well as create a batch of video frames. We have to experiment with `img_idx`, `y`,`z` and normalization such that you get high accuracy.\n",
        "\n",
        "\n",
        "img_idx - indices of images (in each folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQ-hWNGB6dO1"
      },
      "outputs": [],
      "source": [
        "def generator(source_path, folder_list, batch_size):\n",
        "    # print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
        "    img_idx = [0,1,2,4,6,8,10,12,14,16,18,20,22,24,26,27,28,29]#create a list of image numbers we want to use for a particular video\n",
        "    while True:\n",
        "        t = np.random.permutation(folder_list)\n",
        "        num_batches =int(len(t)/batch_size) # calculate the number of batches\n",
        "        for batch in range(num_batches): # we iterate over the number of batches\n",
        "            batch_data = np.zeros((batch_size,x,y,z,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
        "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
        "            for folder in range(batch_size): # iterate over the batch_size\n",
        "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
        "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
        "                    image = io.imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
        "                    #if image.shape == (360,360,3): ###if images are of 360 x 360\n",
        "                    image = resize(image,(y,z,3))\n",
        "                   # else:\n",
        "                        #image = image[:,(image.shape[0] - y ) // 2 : image.shape[0] - (image.shape[0] - y ) // 2] ### if lower quality just centre crop\n",
        "\n",
        "\n",
        "                    #crop the images and resize them. Note that the images are of 2 different shape\n",
        "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
        "\n",
        "                    batch_data[folder,idx,:,:,0] = image[:,:,0]/255.0 #normalise and feed in the image\n",
        "                    batch_data[folder,idx,:,:,1] = image[:,:,1]/255.0#normalise and feed in the image\n",
        "                    batch_data[folder,idx,:,:,2] = image[:,:,2]/255.0#normalise and feed in the image\n",
        "\n",
        "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
        "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
        "\n",
        "\n",
        "        # write the code for the remaining data points which are left after full batches\n",
        "        if (len(t)%batch_size)!=0:\n",
        "          for batch in range(num_batches): # we iterate over the number of batches\n",
        "            batch_data = np.zeros((len(t)%batch_size,x,y,z,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
        "            batch_labels = np.zeros((len(t)%batch_size,5)) # batch_labels is the one hot representation of the output\n",
        "            for folder in range(len(t)%batch_size): # iterate over the batch_size\n",
        "                imgs = os.listdir(source_path+'/'+ t[folder + (num_batches*batch_size)].split(';')[0]) # read all the images in the folder\n",
        "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
        "                    image = io.imread(source_path+'/'+ t[folder + (num_batches*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
        "                    #if image.shape == (360,360,3): ###if images are of 360 x 360\n",
        "                    image = resize(image,(y,z,3))\n",
        "                    #else:\n",
        "                        #image = image[:,(image.shape[0] - y ) // 2 : image.shape[0] - (image.shape[0] - y ) // 2] ### if lower quality just centre crop\n",
        "\n",
        "                    #crop the images and resize them. Note that the images are of 2 different shape\n",
        "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
        "\n",
        "                    batch_data[folder,idx,:,:,0] = image[:,:,0]/255.0 #normalise and feed in the image\n",
        "                    batch_data[folder,idx,:,:,1] = image[:,:,1]/255.0#normalise and feed in the image\n",
        "                    batch_data[folder,idx,:,:,2] = image[:,:,2]/255.0#normalise and feed in the image\n",
        "\n",
        "                batch_labels[folder, int(t[folder + (num_batches*batch_size)].strip().split(';')[2])] = 1\n",
        "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3sSFjv26dO2"
      },
      "source": [
        "Note here that a video is represented above in the generator as (number of images, height, width, number of channels). Take this into consideration while creating the model architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7tM5EMM6dO3",
        "outputId": "4019f576-85fb-44fb-a55e-5e7c059b6ffc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# training sequences = 663\n",
            "# validation sequences = 100\n",
            "# epochs = 50\n"
          ]
        }
      ],
      "source": [
        "# note these paths are particular to google colab session\n",
        "curr_dt_time = datetime.datetime.now()\n",
        "train_path = '/content/tmp/Project_data/train'\n",
        "val_path = '/content/tmp/Project_data/val'\n",
        "num_train_sequences = len(train_doc)\n",
        "print('# training sequences =', num_train_sequences)\n",
        "num_val_sequences = len(val_doc)\n",
        "print('# validation sequences =', num_val_sequences)\n",
        "num_epochs = 50 # choose the number of epochs\n",
        "print ('# epochs =', num_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aTx-hSP6dO4"
      },
      "source": [
        "## Model\n",
        "Here you make the model using different functionalities that Keras provides. Remember to use `Conv3D` and `MaxPooling3D` and not `Conv2D` and `Maxpooling2D` for a 3D convolution model. You would want to use `TimeDistributed` while building a Conv2D + RNN model. Also remember that the last layer is the softmax. Design the network in such a way that the model is able to give good accuracy on the least number of parameters so that it can fit in the memory of the webcam."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**This is the first model (Conv3d) mentioned in Write-up file**"
      ],
      "metadata": {
        "id": "R307G4gwG3-C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHnd2XWD6dO4"
      },
      "outputs": [],
      "source": [
        "# libraries required for model architecture\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation,Dropout\n",
        "from keras.layers import Conv3D, MaxPooling3D\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras import optimizers\n",
        "import keras\n",
        "\n",
        "#write our model here\n",
        "model=Sequential()\n",
        "# first conv3d block\n",
        "model.add(Conv3D(32,(3,3,3),strides=(1,1,1),padding=\"same\",input_shape=(18,84,84,3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation(\"elu\"))\n",
        "model.add(MaxPooling3D(pool_size=(2,2,1),strides=(2,2,1)))\n",
        "# second conv3d block\n",
        "model.add(Conv3D(128,(3,3,3),strides=(1,1,1),padding=\"same\"))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation(\"elu\"))\n",
        "model.add(MaxPooling3D(pool_size=(2,2,2),strides=(2,2,2)))\n",
        "# third conv3d block\n",
        "model.add(Conv3D(256,(3,3,3),strides=(1,1,1),padding=\"same\"))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation(\"elu\"))\n",
        "model.add(MaxPooling3D(pool_size=(2,2,2),strides=(2,2,2)))\n",
        "# let's flatten and use dropout as the regularizer\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(512,activation=\"elu\"))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(5,activation=\"softmax\")) # 5 because there are 5 classes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEIJlSxq6dO4"
      },
      "source": [
        "Now that we have written the model, the next step is to `compile` the model. When we print the `summary` of the model, we'll see the total number of parameters we have to train."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GswJrQ8O6dO5",
        "outputId": "b51d1a2c-aeec-4ee3-8569-3f9e67814f31",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv3d (Conv3D)             (None, 18, 84, 84, 32)    2624      \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 18, 84, 84, 32)    128       \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " activation (Activation)     (None, 18, 84, 84, 32)    0         \n",
            "                                                                 \n",
            " max_pooling3d (MaxPooling3  (None, 9, 42, 84, 32)     0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv3d_1 (Conv3D)           (None, 9, 42, 84, 128)    110720    \n",
            "                                                                 \n",
            " batch_normalization_1 (Bat  (None, 9, 42, 84, 128)    512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 9, 42, 84, 128)    0         \n",
            "                                                                 \n",
            " max_pooling3d_1 (MaxPoolin  (None, 4, 21, 42, 128)    0         \n",
            " g3D)                                                            \n",
            "                                                                 \n",
            " conv3d_2 (Conv3D)           (None, 4, 21, 42, 256)    884992    \n",
            "                                                                 \n",
            " batch_normalization_2 (Bat  (None, 4, 21, 42, 256)    1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 4, 21, 42, 256)    0         \n",
            "                                                                 \n",
            " max_pooling3d_2 (MaxPoolin  (None, 2, 10, 21, 256)    0         \n",
            " g3D)                                                            \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 107520)            0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 107520)            0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               55050752  \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 5)                 2565      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 56053317 (213.83 MB)\n",
            "Trainable params: 56052485 (213.82 MB)\n",
            "Non-trainable params: 832 (3.25 KB)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "# optimiser and using categorical accuracy as metric\n",
        "optimiser = tf.keras.optimizers.legacy.SGD(learning_rate=0.001,decay=1e-6,momentum=0.7, nesterov=True)#write your optimizer\n",
        "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "print (model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAxV9DIu6dO5"
      },
      "source": [
        "Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ayZ92nX6dO5"
      },
      "outputs": [],
      "source": [
        "# generating training and validation generator to be fed in training\n",
        "train_generator = generator(train_path, train_doc, batch_size)\n",
        "val_generator = generator(val_path, val_doc, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVOg8Bjn6dO6",
        "outputId": "e00409f0-2c6a-4fc3-9154-b905a004131f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ]
        }
      ],
      "source": [
        "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
        "\n",
        "if not os.path.exists('/content/tmp/Model1st/'+model_name):\n",
        "    os.makedirs('/content/tmp/Model1st/'+model_name, exist_ok=True)\n",
        "\n",
        "filepath = \"/content/tmp/Model1st/\"+model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
        "\n",
        "# saving best model and it's weights only\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True, mode='auto', period=1)\n",
        "# reducing LR based on val loss\n",
        "LR =ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=4) # write the REducelronplateau code here\n",
        "callbacks_list = [checkpoint, LR]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "423rWvk56dO6"
      },
      "source": [
        "The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Y6Pojam6dO6"
      },
      "outputs": [],
      "source": [
        "# deriving steps per epoch based on number of training videos/folder and batch size\n",
        "if (num_train_sequences%batch_size) == 0:\n",
        "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
        "else:\n",
        "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
        "\n",
        "if (num_val_sequences%batch_size) == 0:\n",
        "    validation_steps = int(num_val_sequences/batch_size)\n",
        "else:\n",
        "    validation_steps = (num_val_sequences//batch_size) + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpulRBsA6dO6"
      },
      "source": [
        "Let us now fit the model. This will start training the model and with the help of the checkpoints, we'll be able to save the best model based on val loss (so far)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZhfzPzO6dO6",
        "outputId": "cc77f762-afed-4e8d-a7d7-0c392e0a531d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-7f8f9d39394d>:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 6.4506 - categorical_accuracy: 0.3454\n",
            "Epoch 1: val_loss improved from inf to 3.62325, saving model to /content/tmp/Model1st/model_init_2023-12-1012_19_00.095192/model-00001-6.45061-0.34540-3.62325-0.18000.h5\n",
            "21/21 [==============================] - 143s 6s/step - loss: 6.4506 - categorical_accuracy: 0.3454 - val_loss: 3.6233 - val_categorical_accuracy: 0.1800 - lr: 0.0010\n",
            "Epoch 2/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.7614 - categorical_accuracy: 0.7405\n",
            "Epoch 2: val_loss improved from 3.62325 to 2.16544, saving model to /content/tmp/Model1st/model_init_2023-12-1012_19_00.095192/model-00002-0.76136-0.74052-2.16544-0.31000.h5\n",
            "21/21 [==============================] - 105s 5s/step - loss: 0.7614 - categorical_accuracy: 0.7405 - val_loss: 2.1654 - val_categorical_accuracy: 0.3100 - lr: 0.0010\n",
            "Epoch 3/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 1.5353 - categorical_accuracy: 0.4853\n",
            "Epoch 3: val_loss did not improve from 2.16544\n",
            "21/21 [==============================] - 108s 5s/step - loss: 1.5353 - categorical_accuracy: 0.4853 - val_loss: 4.2348 - val_categorical_accuracy: 0.2222 - lr: 0.0010\n",
            "Epoch 4/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.6390 - categorical_accuracy: 0.8054\n",
            "Epoch 4: val_loss did not improve from 2.16544\n",
            "21/21 [==============================] - 89s 4s/step - loss: 0.6390 - categorical_accuracy: 0.8054 - val_loss: 6.0495 - val_categorical_accuracy: 0.2727 - lr: 0.0010\n",
            "Epoch 5/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 1.1900 - categorical_accuracy: 0.5917\n",
            "Epoch 5: val_loss did not improve from 2.16544\n",
            "21/21 [==============================] - 102s 5s/step - loss: 1.1900 - categorical_accuracy: 0.5917 - val_loss: 6.8869 - val_categorical_accuracy: 0.2273 - lr: 0.0010\n",
            "Epoch 6/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.5212 - categorical_accuracy: 0.8138\n",
            "Epoch 6: val_loss did not improve from 2.16544\n",
            "\n",
            "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "21/21 [==============================] - 98s 5s/step - loss: 0.5212 - categorical_accuracy: 0.8138 - val_loss: 8.4849 - val_categorical_accuracy: 0.3056 - lr: 0.0010\n",
            "Epoch 7/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.9068 - categorical_accuracy: 0.6765\n",
            "Epoch 7: val_loss did not improve from 2.16544\n",
            "21/21 [==============================] - 123s 6s/step - loss: 0.9068 - categorical_accuracy: 0.6765 - val_loss: 8.9185 - val_categorical_accuracy: 0.2700 - lr: 2.0000e-04\n",
            "Epoch 8/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.6180 - categorical_accuracy: 0.7802\n",
            "Epoch 8: val_loss did not improve from 2.16544\n",
            "21/21 [==============================] - 102s 5s/step - loss: 0.6180 - categorical_accuracy: 0.7802 - val_loss: 9.4154 - val_categorical_accuracy: 0.2400 - lr: 2.0000e-04\n",
            "Epoch 9/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.8314 - categorical_accuracy: 0.7039\n",
            "Epoch 9: val_loss did not improve from 2.16544\n",
            "21/21 [==============================] - 106s 5s/step - loss: 0.8314 - categorical_accuracy: 0.7039 - val_loss: 8.8159 - val_categorical_accuracy: 0.3056 - lr: 2.0000e-04\n",
            "Epoch 10/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.6435 - categorical_accuracy: 0.7976\n",
            "Epoch 10: val_loss did not improve from 2.16544\n",
            "\n",
            "Epoch 10: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "21/21 [==============================] - 87s 4s/step - loss: 0.6435 - categorical_accuracy: 0.7976 - val_loss: 5.7219 - val_categorical_accuracy: 0.4091 - lr: 2.0000e-04\n",
            "Epoch 11/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.7012 - categorical_accuracy: 0.7435\n",
            "Epoch 11: val_loss did not improve from 2.16544\n",
            "21/21 [==============================] - 98s 5s/step - loss: 0.7012 - categorical_accuracy: 0.7435 - val_loss: 9.5756 - val_categorical_accuracy: 0.2045 - lr: 4.0000e-05\n",
            "Epoch 12/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.7602 - categorical_accuracy: 0.7445\n",
            "Epoch 12: val_loss did not improve from 2.16544\n",
            "21/21 [==============================] - 116s 6s/step - loss: 0.7602 - categorical_accuracy: 0.7445 - val_loss: 7.5891 - val_categorical_accuracy: 0.3056 - lr: 4.0000e-05\n",
            "Epoch 13/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.5102 - categorical_accuracy: 0.8126\n",
            "Epoch 13: val_loss did not improve from 2.16544\n",
            "21/21 [==============================] - 107s 5s/step - loss: 0.5102 - categorical_accuracy: 0.8126 - val_loss: 8.1159 - val_categorical_accuracy: 0.2800 - lr: 4.0000e-05\n",
            "Epoch 14/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.6225 - categorical_accuracy: 0.7767\n",
            "Epoch 14: val_loss did not improve from 2.16544\n",
            "\n",
            "Epoch 14: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
            "21/21 [==============================] - 116s 6s/step - loss: 0.6225 - categorical_accuracy: 0.7767 - val_loss: 7.6493 - val_categorical_accuracy: 0.2800 - lr: 4.0000e-05\n",
            "Epoch 15/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.7086 - categorical_accuracy: 0.7318\n",
            "Epoch 15: val_loss did not improve from 2.16544\n",
            "21/21 [==============================] - 82s 4s/step - loss: 0.7086 - categorical_accuracy: 0.7318 - val_loss: 7.8781 - val_categorical_accuracy: 0.2639 - lr: 8.0000e-06\n",
            "Epoch 16/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.6227 - categorical_accuracy: 0.7528\n",
            "Epoch 16: val_loss did not improve from 2.16544\n",
            "21/21 [==============================] - 101s 5s/step - loss: 0.6227 - categorical_accuracy: 0.7528 - val_loss: 6.6709 - val_categorical_accuracy: 0.3182 - lr: 8.0000e-06\n",
            "Epoch 17/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.6883 - categorical_accuracy: 0.7380\n",
            "Epoch 17: val_loss did not improve from 2.16544\n",
            "21/21 [==============================] - 84s 4s/step - loss: 0.6883 - categorical_accuracy: 0.7380 - val_loss: 5.7407 - val_categorical_accuracy: 0.2045 - lr: 8.0000e-06\n",
            "Epoch 18/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.5969 - categorical_accuracy: 0.7674\n",
            "Epoch 18: val_loss did not improve from 2.16544\n",
            "\n",
            "Epoch 18: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
            "21/21 [==============================] - 104s 5s/step - loss: 0.5969 - categorical_accuracy: 0.7674 - val_loss: 5.4945 - val_categorical_accuracy: 0.2361 - lr: 8.0000e-06\n",
            "Epoch 19/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.7327 - categorical_accuracy: 0.7166\n",
            "Epoch 19: val_loss did not improve from 2.16544\n",
            "21/21 [==============================] - 103s 5s/step - loss: 0.7327 - categorical_accuracy: 0.7166 - val_loss: 4.3292 - val_categorical_accuracy: 0.3200 - lr: 1.6000e-06\n",
            "Epoch 20/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.6617 - categorical_accuracy: 0.7481\n",
            "Epoch 20: val_loss did not improve from 2.16544\n",
            "21/21 [==============================] - 109s 5s/step - loss: 0.6617 - categorical_accuracy: 0.7481 - val_loss: 3.7599 - val_categorical_accuracy: 0.3400 - lr: 1.6000e-06\n",
            "Epoch 21/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.5701 - categorical_accuracy: 0.7724\n",
            "Epoch 21: val_loss did not improve from 2.16544\n",
            "21/21 [==============================] - 96s 5s/step - loss: 0.5701 - categorical_accuracy: 0.7724 - val_loss: 3.2467 - val_categorical_accuracy: 0.3611 - lr: 1.6000e-06\n",
            "Epoch 22/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.6444 - categorical_accuracy: 0.7737\n",
            "Epoch 22: val_loss improved from 2.16544 to 1.95956, saving model to /content/tmp/Model1st/model_init_2023-12-1012_19_00.095192/model-00022-0.64436-0.77370-1.95956-0.45455.h5\n",
            "21/21 [==============================] - 95s 5s/step - loss: 0.6444 - categorical_accuracy: 0.7737 - val_loss: 1.9596 - val_categorical_accuracy: 0.4545 - lr: 1.6000e-06\n",
            "Epoch 23/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.6980 - categorical_accuracy: 0.7314\n",
            "Epoch 23: val_loss did not improve from 1.95956\n",
            "21/21 [==============================] - 84s 4s/step - loss: 0.6980 - categorical_accuracy: 0.7314 - val_loss: 2.9004 - val_categorical_accuracy: 0.2727 - lr: 1.6000e-06\n",
            "Epoch 24/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.6971 - categorical_accuracy: 0.7343\n",
            "Epoch 24: val_loss did not improve from 1.95956\n",
            "21/21 [==============================] - 105s 5s/step - loss: 0.6971 - categorical_accuracy: 0.7343 - val_loss: 1.9721 - val_categorical_accuracy: 0.3750 - lr: 1.6000e-06\n",
            "Epoch 25/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.8874 - categorical_accuracy: 0.6648\n",
            "Epoch 25: val_loss improved from 1.95956 to 1.67070, saving model to /content/tmp/Model1st/model_init_2023-12-1012_19_00.095192/model-00025-0.88736-0.66477-1.67070-0.45000.h5\n",
            "21/21 [==============================] - 112s 6s/step - loss: 0.8874 - categorical_accuracy: 0.6648 - val_loss: 1.6707 - val_categorical_accuracy: 0.4500 - lr: 1.6000e-06\n",
            "Epoch 26/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.6859 - categorical_accuracy: 0.7427\n",
            "Epoch 26: val_loss improved from 1.67070 to 1.42545, saving model to /content/tmp/Model1st/model_init_2023-12-1012_19_00.095192/model-00026-0.68592-0.74272-1.42545-0.53000.h5\n",
            "21/21 [==============================] - 108s 5s/step - loss: 0.6859 - categorical_accuracy: 0.7427 - val_loss: 1.4254 - val_categorical_accuracy: 0.5300 - lr: 1.6000e-06\n",
            "Epoch 27/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.5663 - categorical_accuracy: 0.7839\n",
            "Epoch 27: val_loss improved from 1.42545 to 1.28689, saving model to /content/tmp/Model1st/model_init_2023-12-1012_19_00.095192/model-00027-0.56630-0.78388-1.28689-0.52778.h5\n",
            "21/21 [==============================] - 103s 5s/step - loss: 0.5663 - categorical_accuracy: 0.7839 - val_loss: 1.2869 - val_categorical_accuracy: 0.5278 - lr: 1.6000e-06\n",
            "Epoch 28/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.7227 - categorical_accuracy: 0.7167\n",
            "Epoch 28: val_loss improved from 1.28689 to 0.76615, saving model to /content/tmp/Model1st/model_init_2023-12-1012_19_00.095192/model-00028-0.72274-0.71667-0.76615-0.70455.h5\n",
            "21/21 [==============================] - 86s 4s/step - loss: 0.7227 - categorical_accuracy: 0.7167 - val_loss: 0.7661 - val_categorical_accuracy: 0.7045 - lr: 1.6000e-06\n",
            "Epoch 29/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.7089 - categorical_accuracy: 0.7429\n",
            "Epoch 29: val_loss did not improve from 0.76615\n",
            "21/21 [==============================] - 97s 5s/step - loss: 0.7089 - categorical_accuracy: 0.7429 - val_loss: 1.2488 - val_categorical_accuracy: 0.6136 - lr: 1.6000e-06\n",
            "Epoch 30/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.7104 - categorical_accuracy: 0.7302\n",
            "Epoch 30: val_loss did not improve from 0.76615\n",
            "21/21 [==============================] - 95s 5s/step - loss: 0.7104 - categorical_accuracy: 0.7302 - val_loss: 1.1549 - val_categorical_accuracy: 0.5833 - lr: 1.6000e-06\n",
            "Epoch 31/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.7109 - categorical_accuracy: 0.7560\n",
            "Epoch 31: val_loss did not improve from 0.76615\n",
            "21/21 [==============================] - 103s 5s/step - loss: 0.7109 - categorical_accuracy: 0.7560 - val_loss: 0.9463 - val_categorical_accuracy: 0.6600 - lr: 1.6000e-06\n",
            "Epoch 32/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.7728 - categorical_accuracy: 0.7163\n",
            "Epoch 32: val_loss did not improve from 0.76615\n",
            "\n",
            "Epoch 32: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
            "21/21 [==============================] - 105s 5s/step - loss: 0.7728 - categorical_accuracy: 0.7163 - val_loss: 0.9080 - val_categorical_accuracy: 0.6900 - lr: 1.6000e-06\n",
            "Epoch 33/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.6747 - categorical_accuracy: 0.7383\n",
            "Epoch 33: val_loss did not improve from 0.76615\n",
            "21/21 [==============================] - 102s 5s/step - loss: 0.6747 - categorical_accuracy: 0.7383 - val_loss: 0.7818 - val_categorical_accuracy: 0.7222 - lr: 3.2000e-07\n",
            "Epoch 34/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.5945 - categorical_accuracy: 0.7766\n",
            "Epoch 34: val_loss improved from 0.76615 to 0.60149, saving model to /content/tmp/Model1st/model_init_2023-12-1012_19_00.095192/model-00034-0.59447-0.77656-0.60149-0.81818.h5\n",
            "21/21 [==============================] - 80s 4s/step - loss: 0.5945 - categorical_accuracy: 0.7766 - val_loss: 0.6015 - val_categorical_accuracy: 0.8182 - lr: 3.2000e-07\n",
            "Epoch 35/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.6864 - categorical_accuracy: 0.7427\n",
            "Epoch 35: val_loss did not improve from 0.60149\n",
            "21/21 [==============================] - 101s 5s/step - loss: 0.6864 - categorical_accuracy: 0.7427 - val_loss: 0.8983 - val_categorical_accuracy: 0.5682 - lr: 3.2000e-07\n",
            "Epoch 36/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.6346 - categorical_accuracy: 0.7538\n",
            "Epoch 36: val_loss did not improve from 0.60149\n",
            "21/21 [==============================] - 92s 5s/step - loss: 0.6346 - categorical_accuracy: 0.7538 - val_loss: 0.8753 - val_categorical_accuracy: 0.7083 - lr: 3.2000e-07\n",
            "Epoch 37/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.6070 - categorical_accuracy: 0.7610\n",
            "Epoch 37: val_loss did not improve from 0.60149\n",
            "21/21 [==============================] - 116s 6s/step - loss: 0.6070 - categorical_accuracy: 0.7610 - val_loss: 0.8195 - val_categorical_accuracy: 0.7100 - lr: 3.2000e-07\n",
            "Epoch 38/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.7188 - categorical_accuracy: 0.7549\n",
            "Epoch 38: val_loss did not improve from 0.60149\n",
            "\n",
            "Epoch 38: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n",
            "21/21 [==============================] - 91s 5s/step - loss: 0.7188 - categorical_accuracy: 0.7549 - val_loss: 0.8498 - val_categorical_accuracy: 0.7000 - lr: 3.2000e-07\n",
            "Epoch 39/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.7213 - categorical_accuracy: 0.7187\n",
            "Epoch 39: val_loss did not improve from 0.60149\n",
            "21/21 [==============================] - 114s 6s/step - loss: 0.7213 - categorical_accuracy: 0.7187 - val_loss: 0.7100 - val_categorical_accuracy: 0.7500 - lr: 6.4000e-08\n",
            "Epoch 40/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.6313 - categorical_accuracy: 0.7317\n",
            "Epoch 40: val_loss did not improve from 0.60149\n",
            "21/21 [==============================] - 82s 4s/step - loss: 0.6313 - categorical_accuracy: 0.7317 - val_loss: 0.8035 - val_categorical_accuracy: 0.7273 - lr: 6.4000e-08\n",
            "Epoch 41/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.6688 - categorical_accuracy: 0.7376\n",
            "Epoch 41: val_loss did not improve from 0.60149\n",
            "21/21 [==============================] - 114s 6s/step - loss: 0.6688 - categorical_accuracy: 0.7376 - val_loss: 0.6623 - val_categorical_accuracy: 0.7955 - lr: 6.4000e-08\n",
            "Epoch 42/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.6894 - categorical_accuracy: 0.7186\n",
            "Epoch 42: val_loss did not improve from 0.60149\n",
            "\n",
            "Epoch 42: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.\n",
            "21/21 [==============================] - 70s 3s/step - loss: 0.6894 - categorical_accuracy: 0.7186 - val_loss: 0.8384 - val_categorical_accuracy: 0.6944 - lr: 6.4000e-08\n",
            "Epoch 43/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.6592 - categorical_accuracy: 0.7535\n",
            "Epoch 43: val_loss did not improve from 0.60149\n",
            "21/21 [==============================] - 118s 6s/step - loss: 0.6592 - categorical_accuracy: 0.7535 - val_loss: 0.8141 - val_categorical_accuracy: 0.7100 - lr: 1.2800e-08\n",
            "Epoch 44/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.8278 - categorical_accuracy: 0.7148\n",
            "Epoch 44: val_loss did not improve from 0.60149\n",
            "21/21 [==============================] - 110s 5s/step - loss: 0.8278 - categorical_accuracy: 0.7148 - val_loss: 0.8068 - val_categorical_accuracy: 0.7200 - lr: 1.2800e-08\n",
            "Epoch 45/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.6508 - categorical_accuracy: 0.7544\n",
            "Epoch 45: val_loss did not improve from 0.60149\n",
            "21/21 [==============================] - 98s 5s/step - loss: 0.6508 - categorical_accuracy: 0.7544 - val_loss: 0.6168 - val_categorical_accuracy: 0.7778 - lr: 1.2800e-08\n",
            "Epoch 46/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.5939 - categorical_accuracy: 0.7747\n",
            "Epoch 46: val_loss did not improve from 0.60149\n",
            "\n",
            "Epoch 46: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.\n",
            "21/21 [==============================] - 82s 4s/step - loss: 0.5939 - categorical_accuracy: 0.7747 - val_loss: 1.0102 - val_categorical_accuracy: 0.6591 - lr: 1.2800e-08\n",
            "Epoch 47/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.7698 - categorical_accuracy: 0.7241\n",
            "Epoch 47: val_loss did not improve from 0.60149\n",
            "21/21 [==============================] - 101s 5s/step - loss: 0.7698 - categorical_accuracy: 0.7241 - val_loss: 0.8330 - val_categorical_accuracy: 0.7955 - lr: 2.5600e-09\n",
            "Epoch 48/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.7726 - categorical_accuracy: 0.7135\n",
            "Epoch 48: val_loss did not improve from 0.60149\n",
            "21/21 [==============================] - 114s 6s/step - loss: 0.7726 - categorical_accuracy: 0.7135 - val_loss: 0.8984 - val_categorical_accuracy: 0.6389 - lr: 2.5600e-09\n",
            "Epoch 49/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.7169 - categorical_accuracy: 0.7242\n",
            "Epoch 49: val_loss did not improve from 0.60149\n",
            "21/21 [==============================] - 94s 5s/step - loss: 0.7169 - categorical_accuracy: 0.7242 - val_loss: 0.8155 - val_categorical_accuracy: 0.7100 - lr: 2.5600e-09\n",
            "Epoch 50/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.6775 - categorical_accuracy: 0.7522\n",
            "Epoch 50: val_loss did not improve from 0.60149\n",
            "\n",
            "Epoch 50: ReduceLROnPlateau reducing learning rate to 5.1200004236307e-10.\n",
            "21/21 [==============================] - 98s 5s/step - loss: 0.6775 - categorical_accuracy: 0.7522 - val_loss: 0.8474 - val_categorical_accuracy: 0.7000 - lr: 2.5600e-09\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7beb58393e80>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "#fitting training and validation generator to model\n",
        "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n",
        "                    callbacks=callbacks_list, validation_data=val_generator,\n",
        "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Note - Code below is repeatative but model architecture is different.. it's just for experimenting purpose."
      ],
      "metadata": {
        "id": "GjPSlab_Ggjq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**This is the fourth model (VGG - Conv2d - LSTM) mentioned in Write-up file**\n",
        "The other two architectures are mentioned at the end of notebook"
      ],
      "metadata": {
        "id": "HzWNKduaHEg_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mlzvqiap6dO7",
        "outputId": "06baba92-d8e3-44eb-a07e-a5e2df9ccc96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58889256/58889256 [==============================] - 2s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Although no need to import all the libraries again and again but\n",
        "# it's just because we experimented with various sets of models at different times..\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, GRU, LSTM, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation,Dropout\n",
        "from keras.layers import Conv3D, MaxPooling3D\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras import optimizers\n",
        "from keras.applications.vgg16 import VGG16\n",
        "import keras\n",
        "\n",
        "#write our model here\n",
        "base_model=VGG16(include_top=False,weights='imagenet',input_shape=(84,84,3))\n",
        "for layers in base_model.layers[:-4]:\n",
        "  layers.trainable=False\n",
        "\n",
        "model=Sequential()\n",
        "model.add(TimeDistributed(base_model,input_shape=(18,84,84,3)))\n",
        "model.add(TimeDistributed(Conv2D(1024,(3,3),strides=(1,1),padding=\"same\")))\n",
        "model.add(TimeDistributed(BatchNormalization()))\n",
        "model.add(TimeDistributed(Activation(\"elu\")))\n",
        "model.add(TimeDistributed(MaxPooling2D(pool_size=(2,2),strides=(2,2))))\n",
        "model.add(TimeDistributed(Flatten()))\n",
        "model.add(LSTM(512,return_sequences=True))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(128,activation='relu'))\n",
        "model.add(Dense(5,activation='softmax'))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rVVCVzIWUQQ",
        "outputId": "e5bc08ef-b114-4dde-afed-894a8a892f4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " time_distributed (TimeDist  (None, 18, 2, 2, 512)     14714688  \n",
            " ributed)                                                        \n",
            "                                                                 \n",
            " time_distributed_1 (TimeDi  (None, 18, 2, 2, 1024)    4719616   \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " time_distributed_2 (TimeDi  (None, 18, 2, 2, 1024)    4096      \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " time_distributed_3 (TimeDi  (None, 18, 2, 2, 1024)    0         \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " time_distributed_4 (TimeDi  (None, 18, 1, 1, 1024)    0         \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " time_distributed_5 (TimeDi  (None, 18, 1024)          0         \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 18, 512)           3147776   \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 256)               787456    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 256)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               32896     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 5)                 645       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 23407173 (89.29 MB)\n",
            "Trainable params: 15769861 (60.16 MB)\n",
            "Non-trainable params: 7637312 (29.13 MB)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "# optimiser and using categorical accuracy as our metric\n",
        "optimiser = tf.keras.optimizers.legacy.SGD(lr=0.001,decay=1e-6,momentum=0.7, nesterov=True)#write your optimizer\n",
        "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "print (model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBCf_QQEUaGX"
      },
      "outputs": [],
      "source": [
        "# creating training and validation generator to be fed in training\n",
        "train_generator = generator(train_path, train_doc, batch_size)\n",
        "val_generator = generator(val_path, val_doc, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPdNCL4DUaw3",
        "outputId": "4d13f0a5-0d43-4ac1-fb55-b508b37b0d81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ]
        }
      ],
      "source": [
        "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
        "\n",
        "if not os.path.exists(\"/content/tmp/Conv2DLSTM/\"+model_name):\n",
        "    os.makedirs(\"/content/tmp/Conv2DLSTM/\"+model_name, exist_ok=True)\n",
        "\n",
        "filepath = \"/content/tmp/Conv2DLSTM/\"+model_name + 'conv2d-lstm-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
        "# saving best model and it's weights only\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True, mode='auto', period=1)\n",
        "# reducing LR based on validation loss\n",
        "LR =ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose=1, patience=4) # write the REducelronplateau code here\n",
        "callbacks_list = [checkpoint, LR]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WknCm6NzUbQh"
      },
      "outputs": [],
      "source": [
        "# deriving steps per epoch based on number of training videos/folder and batch size\n",
        "if (num_train_sequences%batch_size) == 0:\n",
        "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
        "else:\n",
        "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
        "\n",
        "if (num_val_sequences%batch_size) == 0:\n",
        "    validation_steps = int(num_val_sequences/batch_size)\n",
        "else:\n",
        "    validation_steps = (num_val_sequences//batch_size) + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GeT_5iTvUhL0",
        "outputId": "b6f8862c-329f-4c72-ddac-a1d33ad9c7d8"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-13-7f8f9d39394d>:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 1.6179 - categorical_accuracy: 0.2459\n",
            "Epoch 1: val_loss improved from inf to 1.57553, saving model to /content/tmp/Conv2DLSTM/model_init_2023-12-1214_38_03.159563/conv2d-lstm-00001-1.61786-0.24585-1.57553-0.32000.h5\n",
            "21/21 [==============================] - 157s 6s/step - loss: 1.6179 - categorical_accuracy: 0.2459 - val_loss: 1.5755 - val_categorical_accuracy: 0.3200 - lr: 0.0010\n",
            "Epoch 2/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 1.2067 - categorical_accuracy: 0.6906\n",
            "Epoch 2: val_loss improved from 1.57553 to 1.54259, saving model to /content/tmp/Conv2DLSTM/model_init_2023-12-1214_38_03.159563/conv2d-lstm-00002-1.20667-0.69062-1.54259-0.33000.h5\n",
            "21/21 [==============================] - 96s 5s/step - loss: 1.2067 - categorical_accuracy: 0.6906 - val_loss: 1.5426 - val_categorical_accuracy: 0.3300 - lr: 0.0010\n",
            "Epoch 3/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 1.3578 - categorical_accuracy: 0.4775\n",
            "Epoch 3: val_loss improved from 1.54259 to 1.49080, saving model to /content/tmp/Conv2DLSTM/model_init_2023-12-1214_38_03.159563/conv2d-lstm-00003-1.35778-0.47752-1.49080-0.40000.h5\n",
            "21/21 [==============================] - 118s 6s/step - loss: 1.3578 - categorical_accuracy: 0.4775 - val_loss: 1.4908 - val_categorical_accuracy: 0.4000 - lr: 0.0010\n",
            "Epoch 4/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.8525 - categorical_accuracy: 0.8266\n",
            "Epoch 4: val_loss did not improve from 1.49080\n",
            "21/21 [==============================] - 109s 5s/step - loss: 0.8525 - categorical_accuracy: 0.8266 - val_loss: 1.5138 - val_categorical_accuracy: 0.2361 - lr: 0.0010\n",
            "Epoch 5/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 1.0723 - categorical_accuracy: 0.6284\n",
            "Epoch 5: val_loss improved from 1.49080 to 1.32694, saving model to /content/tmp/Conv2DLSTM/model_init_2023-12-1214_38_03.159563/conv2d-lstm-00005-1.07226-0.62839-1.32694-0.61364.h5\n",
            "21/21 [==============================] - 95s 5s/step - loss: 1.0723 - categorical_accuracy: 0.6284 - val_loss: 1.3269 - val_categorical_accuracy: 0.6136 - lr: 0.0010\n",
            "Epoch 6/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.6300 - categorical_accuracy: 0.8864\n",
            "Epoch 6: val_loss improved from 1.32694 to 1.12499, saving model to /content/tmp/Conv2DLSTM/model_init_2023-12-1214_38_03.159563/conv2d-lstm-00006-0.62998-0.88641-1.12499-0.59091.h5\n",
            "21/21 [==============================] - 99s 5s/step - loss: 0.6300 - categorical_accuracy: 0.8864 - val_loss: 1.1250 - val_categorical_accuracy: 0.5909 - lr: 0.0010\n",
            "Epoch 7/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.7780 - categorical_accuracy: 0.7570\n",
            "Epoch 7: val_loss did not improve from 1.12499\n",
            "21/21 [==============================] - 123s 6s/step - loss: 0.7780 - categorical_accuracy: 0.7570 - val_loss: 1.1363 - val_categorical_accuracy: 0.5417 - lr: 0.0010\n",
            "Epoch 8/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.4705 - categorical_accuracy: 0.8955\n",
            "Epoch 8: val_loss improved from 1.12499 to 1.03060, saving model to /content/tmp/Conv2DLSTM/model_init_2023-12-1214_38_03.159563/conv2d-lstm-00008-0.47053-0.89550-1.03060-0.59000.h5\n",
            "21/21 [==============================] - 92s 5s/step - loss: 0.4705 - categorical_accuracy: 0.8955 - val_loss: 1.0306 - val_categorical_accuracy: 0.5900 - lr: 0.0010\n",
            "Epoch 9/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.5797 - categorical_accuracy: 0.8443\n",
            "Epoch 9: val_loss improved from 1.03060 to 0.97898, saving model to /content/tmp/Conv2DLSTM/model_init_2023-12-1214_38_03.159563/conv2d-lstm-00009-0.57974-0.84433-0.97898-0.69000.h5\n",
            "21/21 [==============================] - 118s 6s/step - loss: 0.5797 - categorical_accuracy: 0.8443 - val_loss: 0.9790 - val_categorical_accuracy: 0.6900 - lr: 0.0010\n",
            "Epoch 10/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.3681 - categorical_accuracy: 0.9197\n",
            "Epoch 10: val_loss improved from 0.97898 to 0.90427, saving model to /content/tmp/Conv2DLSTM/model_init_2023-12-1214_38_03.159563/conv2d-lstm-00010-0.36814-0.91972-0.90427-0.65278.h5\n",
            "21/21 [==============================] - 110s 5s/step - loss: 0.3681 - categorical_accuracy: 0.9197 - val_loss: 0.9043 - val_categorical_accuracy: 0.6528 - lr: 0.0010\n",
            "Epoch 11/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.3770 - categorical_accuracy: 0.9180\n",
            "Epoch 11: val_loss improved from 0.90427 to 0.80984, saving model to /content/tmp/Conv2DLSTM/model_init_2023-12-1214_38_03.159563/conv2d-lstm-00011-0.37704-0.91798-0.80984-0.77273.h5\n",
            "21/21 [==============================] - 86s 4s/step - loss: 0.3770 - categorical_accuracy: 0.9180 - val_loss: 0.8098 - val_categorical_accuracy: 0.7727 - lr: 0.0010\n",
            "Epoch 12/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.3005 - categorical_accuracy: 0.9323\n",
            "Epoch 12: val_loss did not improve from 0.80984\n",
            "21/21 [==============================] - 101s 5s/step - loss: 0.3005 - categorical_accuracy: 0.9323 - val_loss: 0.8294 - val_categorical_accuracy: 0.6136 - lr: 0.0010\n",
            "Epoch 13/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.2231 - categorical_accuracy: 0.9676\n",
            "Epoch 13: val_loss did not improve from 0.80984\n",
            "21/21 [==============================] - 129s 6s/step - loss: 0.2231 - categorical_accuracy: 0.9676 - val_loss: 1.3514 - val_categorical_accuracy: 0.5417 - lr: 0.0010\n",
            "Epoch 14/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.2130 - categorical_accuracy: 0.9639\n",
            "Epoch 14: val_loss improved from 0.80984 to 0.75048, saving model to /content/tmp/Conv2DLSTM/model_init_2023-12-1214_38_03.159563/conv2d-lstm-00014-0.21296-0.96388-0.75048-0.70000.h5\n",
            "21/21 [==============================] - 112s 6s/step - loss: 0.2130 - categorical_accuracy: 0.9639 - val_loss: 0.7505 - val_categorical_accuracy: 0.7000 - lr: 0.0010\n",
            "Epoch 15/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.1610 - categorical_accuracy: 0.9739\n",
            "Epoch 15: val_loss did not improve from 0.75048\n",
            "21/21 [==============================] - 93s 5s/step - loss: 0.1610 - categorical_accuracy: 0.9739 - val_loss: 0.9737 - val_categorical_accuracy: 0.6000 - lr: 0.0010\n",
            "Epoch 16/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.1588 - categorical_accuracy: 0.9872\n",
            "Epoch 16: val_loss did not improve from 0.75048\n",
            "21/21 [==============================] - 107s 5s/step - loss: 0.1588 - categorical_accuracy: 0.9872 - val_loss: 0.7861 - val_categorical_accuracy: 0.6806 - lr: 0.0010\n",
            "Epoch 17/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.1173 - categorical_accuracy: 0.9942\n",
            "Epoch 17: val_loss improved from 0.75048 to 0.66343, saving model to /content/tmp/Conv2DLSTM/model_init_2023-12-1214_38_03.159563/conv2d-lstm-00017-0.11728-0.99422-0.66343-0.68182.h5\n",
            "21/21 [==============================] - 72s 4s/step - loss: 0.1173 - categorical_accuracy: 0.9942 - val_loss: 0.6634 - val_categorical_accuracy: 0.6818 - lr: 0.0010\n",
            "Epoch 18/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.1265 - categorical_accuracy: 0.9876\n",
            "Epoch 18: val_loss did not improve from 0.66343\n",
            "21/21 [==============================] - 109s 5s/step - loss: 0.1265 - categorical_accuracy: 0.9876 - val_loss: 0.8751 - val_categorical_accuracy: 0.7045 - lr: 0.0010\n",
            "Epoch 19/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0638 - categorical_accuracy: 1.0000\n",
            "Epoch 19: val_loss did not improve from 0.66343\n",
            "21/21 [==============================] - 86s 4s/step - loss: 0.0638 - categorical_accuracy: 1.0000 - val_loss: 0.8050 - val_categorical_accuracy: 0.6944 - lr: 0.0010\n",
            "Epoch 20/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0951 - categorical_accuracy: 0.9955\n",
            "Epoch 20: val_loss did not improve from 0.66343\n",
            "21/21 [==============================] - 131s 7s/step - loss: 0.0951 - categorical_accuracy: 0.9955 - val_loss: 0.7411 - val_categorical_accuracy: 0.7300 - lr: 0.0010\n",
            "Epoch 21/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0510 - categorical_accuracy: 0.9959\n",
            "Epoch 21: val_loss did not improve from 0.66343\n",
            "\n",
            "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "21/21 [==============================] - 88s 4s/step - loss: 0.0510 - categorical_accuracy: 0.9959 - val_loss: 0.7173 - val_categorical_accuracy: 0.7000 - lr: 0.0010\n",
            "Epoch 22/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0776 - categorical_accuracy: 0.9939\n",
            "Epoch 22: val_loss improved from 0.66343 to 0.66295, saving model to /content/tmp/Conv2DLSTM/model_init_2023-12-1214_38_03.159563/conv2d-lstm-00022-0.07762-0.99388-0.66295-0.77778.h5\n",
            "21/21 [==============================] - 108s 5s/step - loss: 0.0776 - categorical_accuracy: 0.9939 - val_loss: 0.6629 - val_categorical_accuracy: 0.7778 - lr: 2.0000e-04\n",
            "Epoch 23/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0498 - categorical_accuracy: 1.0000\n",
            "Epoch 23: val_loss did not improve from 0.66295\n",
            "21/21 [==============================] - 81s 4s/step - loss: 0.0498 - categorical_accuracy: 1.0000 - val_loss: 0.7118 - val_categorical_accuracy: 0.7273 - lr: 2.0000e-04\n",
            "Epoch 24/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0635 - categorical_accuracy: 0.9984\n",
            "Epoch 24: val_loss did not improve from 0.66295\n",
            "21/21 [==============================] - 105s 5s/step - loss: 0.0635 - categorical_accuracy: 0.9984 - val_loss: 0.8177 - val_categorical_accuracy: 0.7273 - lr: 2.0000e-04\n",
            "Epoch 25/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0587 - categorical_accuracy: 0.9981\n",
            "Epoch 25: val_loss did not improve from 0.66295\n",
            "21/21 [==============================] - 108s 5s/step - loss: 0.0587 - categorical_accuracy: 0.9981 - val_loss: 0.7368 - val_categorical_accuracy: 0.7778 - lr: 2.0000e-04\n",
            "Epoch 26/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0617 - categorical_accuracy: 0.9984\n",
            "Epoch 26: val_loss did not improve from 0.66295\n",
            "\n",
            "Epoch 26: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "21/21 [==============================] - 102s 5s/step - loss: 0.0617 - categorical_accuracy: 0.9984 - val_loss: 0.7145 - val_categorical_accuracy: 0.7400 - lr: 2.0000e-04\n",
            "Epoch 27/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0537 - categorical_accuracy: 1.0000\n",
            "Epoch 27: val_loss did not improve from 0.66295\n",
            "21/21 [==============================] - 80s 4s/step - loss: 0.0537 - categorical_accuracy: 1.0000 - val_loss: 0.7558 - val_categorical_accuracy: 0.7400 - lr: 4.0000e-05\n",
            "Epoch 28/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0609 - categorical_accuracy: 0.9983\n",
            "Epoch 28: val_loss improved from 0.66295 to 0.60965, saving model to /content/tmp/Conv2DLSTM/model_init_2023-12-1214_38_03.159563/conv2d-lstm-00028-0.06087-0.99833-0.60965-0.79167.h5\n",
            "21/21 [==============================] - 102s 5s/step - loss: 0.0609 - categorical_accuracy: 0.9983 - val_loss: 0.6096 - val_categorical_accuracy: 0.7917 - lr: 4.0000e-05\n",
            "Epoch 29/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0624 - categorical_accuracy: 0.9982\n",
            "Epoch 29: val_loss did not improve from 0.60965\n",
            "21/21 [==============================] - 77s 4s/step - loss: 0.0624 - categorical_accuracy: 0.9982 - val_loss: 0.7735 - val_categorical_accuracy: 0.7273 - lr: 4.0000e-05\n",
            "Epoch 30/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0541 - categorical_accuracy: 1.0000\n",
            "Epoch 30: val_loss did not improve from 0.60965\n",
            "21/21 [==============================] - 98s 5s/step - loss: 0.0541 - categorical_accuracy: 1.0000 - val_loss: 0.7827 - val_categorical_accuracy: 0.7500 - lr: 4.0000e-05\n",
            "Epoch 31/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0587 - categorical_accuracy: 0.9983\n",
            "Epoch 31: val_loss did not improve from 0.60965\n",
            "21/21 [==============================] - 108s 5s/step - loss: 0.0587 - categorical_accuracy: 0.9983 - val_loss: 0.6295 - val_categorical_accuracy: 0.7778 - lr: 4.0000e-05\n",
            "Epoch 32/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0609 - categorical_accuracy: 0.9982\n",
            "Epoch 32: val_loss did not improve from 0.60965\n",
            "\n",
            "Epoch 32: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
            "21/21 [==============================] - 90s 4s/step - loss: 0.0609 - categorical_accuracy: 0.9982 - val_loss: 0.7221 - val_categorical_accuracy: 0.7600 - lr: 4.0000e-05\n",
            "Epoch 33/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0612 - categorical_accuracy: 0.9967\n",
            "Epoch 33: val_loss did not improve from 0.60965\n",
            "21/21 [==============================] - 105s 5s/step - loss: 0.0612 - categorical_accuracy: 0.9967 - val_loss: 0.6808 - val_categorical_accuracy: 0.7600 - lr: 8.0000e-06\n",
            "Epoch 34/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0625 - categorical_accuracy: 1.0000\n",
            "Epoch 34: val_loss did not improve from 0.60965\n",
            "21/21 [==============================] - 107s 5s/step - loss: 0.0625 - categorical_accuracy: 1.0000 - val_loss: 0.7234 - val_categorical_accuracy: 0.7500 - lr: 8.0000e-06\n",
            "Epoch 35/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0609 - categorical_accuracy: 0.9984\n",
            "Epoch 35: val_loss did not improve from 0.60965\n",
            "21/21 [==============================] - 90s 4s/step - loss: 0.0609 - categorical_accuracy: 0.9984 - val_loss: 0.9417 - val_categorical_accuracy: 0.7273 - lr: 8.0000e-06\n",
            "Epoch 36/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0538 - categorical_accuracy: 1.0000\n",
            "Epoch 36: val_loss did not improve from 0.60965\n",
            "\n",
            "Epoch 36: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
            "21/21 [==============================] - 94s 5s/step - loss: 0.0538 - categorical_accuracy: 1.0000 - val_loss: 0.8020 - val_categorical_accuracy: 0.7273 - lr: 8.0000e-06\n",
            "Epoch 37/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0559 - categorical_accuracy: 0.9984\n",
            "Epoch 37: val_loss did not improve from 0.60965\n",
            "21/21 [==============================] - 126s 6s/step - loss: 0.0559 - categorical_accuracy: 0.9984 - val_loss: 0.8668 - val_categorical_accuracy: 0.7222 - lr: 1.6000e-06\n",
            "Epoch 38/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0602 - categorical_accuracy: 1.0000\n",
            "Epoch 38: val_loss did not improve from 0.60965\n",
            "21/21 [==============================] - 82s 4s/step - loss: 0.0602 - categorical_accuracy: 1.0000 - val_loss: 0.7308 - val_categorical_accuracy: 0.7500 - lr: 1.6000e-06\n",
            "Epoch 39/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0575 - categorical_accuracy: 0.9985\n",
            "Epoch 39: val_loss did not improve from 0.60965\n",
            "21/21 [==============================] - 108s 5s/step - loss: 0.0575 - categorical_accuracy: 0.9985 - val_loss: 0.6665 - val_categorical_accuracy: 0.7700 - lr: 1.6000e-06\n",
            "Epoch 40/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0547 - categorical_accuracy: 1.0000\n",
            "Epoch 40: val_loss did not improve from 0.60965\n",
            "\n",
            "Epoch 40: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
            "21/21 [==============================] - 99s 5s/step - loss: 0.0547 - categorical_accuracy: 1.0000 - val_loss: 0.8154 - val_categorical_accuracy: 0.6944 - lr: 1.6000e-06\n",
            "Epoch 41/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0588 - categorical_accuracy: 0.9970\n",
            "Epoch 41: val_loss did not improve from 0.60965\n",
            "21/21 [==============================] - 96s 5s/step - loss: 0.0588 - categorical_accuracy: 0.9970 - val_loss: 1.0838 - val_categorical_accuracy: 0.7045 - lr: 3.2000e-07\n",
            "Epoch 42/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0647 - categorical_accuracy: 1.0000\n",
            "Epoch 42: val_loss improved from 0.60965 to 0.58542, saving model to /content/tmp/Conv2DLSTM/model_init_2023-12-1214_38_03.159563/conv2d-lstm-00042-0.06474-1.00000-0.58542-0.75000.h5\n",
            "21/21 [==============================] - 65s 3s/step - loss: 0.0647 - categorical_accuracy: 1.0000 - val_loss: 0.5854 - val_categorical_accuracy: 0.7500 - lr: 3.2000e-07\n",
            "Epoch 43/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0524 - categorical_accuracy: 0.9984\n",
            "Epoch 43: val_loss did not improve from 0.58542\n",
            "21/21 [==============================] - 128s 6s/step - loss: 0.0524 - categorical_accuracy: 0.9984 - val_loss: 0.7746 - val_categorical_accuracy: 0.7639 - lr: 3.2000e-07\n",
            "Epoch 44/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0490 - categorical_accuracy: 1.0000\n",
            "Epoch 44: val_loss did not improve from 0.58542\n",
            "21/21 [==============================] - 88s 4s/step - loss: 0.0490 - categorical_accuracy: 1.0000 - val_loss: 0.7253 - val_categorical_accuracy: 0.7600 - lr: 3.2000e-07\n",
            "Epoch 45/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0728 - categorical_accuracy: 0.9920\n",
            "Epoch 45: val_loss did not improve from 0.58542\n",
            "21/21 [==============================] - 114s 6s/step - loss: 0.0728 - categorical_accuracy: 0.9920 - val_loss: 0.7217 - val_categorical_accuracy: 0.7600 - lr: 3.2000e-07\n",
            "Epoch 46/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.1148 - categorical_accuracy: 0.9739\n",
            "Epoch 46: val_loss did not improve from 0.58542\n",
            "\n",
            "Epoch 46: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n",
            "21/21 [==============================] - 79s 4s/step - loss: 0.1148 - categorical_accuracy: 0.9739 - val_loss: 0.7189 - val_categorical_accuracy: 0.7083 - lr: 3.2000e-07\n",
            "Epoch 47/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0602 - categorical_accuracy: 0.9984\n",
            "Epoch 47: val_loss improved from 0.58542 to 0.54686, saving model to /content/tmp/Conv2DLSTM/model_init_2023-12-1214_38_03.159563/conv2d-lstm-00047-0.06019-0.99836-0.54686-0.84091.h5\n",
            "21/21 [==============================] - 92s 5s/step - loss: 0.0602 - categorical_accuracy: 0.9984 - val_loss: 0.5469 - val_categorical_accuracy: 0.8409 - lr: 6.4000e-08\n",
            "Epoch 48/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0580 - categorical_accuracy: 0.9982\n",
            "Epoch 48: val_loss did not improve from 0.54686\n",
            "21/21 [==============================] - 95s 5s/step - loss: 0.0580 - categorical_accuracy: 0.9982 - val_loss: 1.0007 - val_categorical_accuracy: 0.6591 - lr: 6.4000e-08\n",
            "Epoch 49/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0585 - categorical_accuracy: 1.0000\n",
            "Epoch 49: val_loss did not improve from 0.54686\n",
            "21/21 [==============================] - 97s 5s/step - loss: 0.0585 - categorical_accuracy: 1.0000 - val_loss: 0.7400 - val_categorical_accuracy: 0.7361 - lr: 6.4000e-08\n",
            "Epoch 50/50\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0560 - categorical_accuracy: 0.9983\n",
            "Epoch 50: val_loss did not improve from 0.54686\n",
            "21/21 [==============================] - 100s 5s/step - loss: 0.0560 - categorical_accuracy: 0.9983 - val_loss: 0.7196 - val_categorical_accuracy: 0.7500 - lr: 6.4000e-08\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7ada381626b0>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# fitting generators to model\n",
        "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n",
        "                    callbacks=callbacks_list, validation_data=val_generator,\n",
        "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q6QB4CJgHRfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**This is the second model (VGG - GRU) mentioned in Write-up file**"
      ],
      "metadata": {
        "id": "_BsjVCn6HOWa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIdsFdoNWM_u"
      },
      "outputs": [],
      "source": [
        "#write our model here\n",
        "base_model=VGG16(include_top=False,weights='imagenet',input_shape=(84,84,3))\n",
        "\n",
        "for layers in base_model.layers[:-4]:\n",
        "  layers.trainable=False\n",
        "\n",
        "model=Sequential()\n",
        "model.add(TimeDistributed(base_model,input_shape=(18,84,84,3)))\n",
        "model.add(TimeDistributed(Flatten()))\n",
        "model.add(GRU(128,return_sequences=True))\n",
        "model.add(GRU(64))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64,activation='relu'))\n",
        "model.add(Dense(5,activation='softmax'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**This is the third model (VGG - Conv2d - GRU) mentioned in Write-up file**"
      ],
      "metadata": {
        "id": "KbpxbOzZHbC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#write our model here\n",
        "base_model=VGG16(include_top=False,weights='imagenet',input_shape=(84,84,3))\n",
        "\n",
        "for layers in base_model.layers[:-4]:\n",
        "  layers.trainable=False\n",
        "\n",
        "model=Sequential()\n",
        "model.add(TimeDistributed(base_model,input_shape=(18,84,84,3)))\n",
        "model.add(TimeDistributed(Conv2D(128,(3,3),strides=(1,1),padding=\"same\")))\n",
        "model.add(TimeDistributed(BatchNormalization()))\n",
        "model.add(TimeDistributed(Activation(\"elu\")))\n",
        "model.add(TimeDistributed(MaxPooling2D(pool_size=(2,2),strides=(2,2))))\n",
        "model.add(TimeDistributed(Flatten()))\n",
        "model.add(GRU(128,return_sequences=True))\n",
        "model.add(GRU(64))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64,activation='relu'))\n",
        "model.add(Dense(5,activation='softmax'))"
      ],
      "metadata": {
        "id": "IkCQ7Z0IHfoQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}